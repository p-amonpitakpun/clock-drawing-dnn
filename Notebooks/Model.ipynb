{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.io import loadmat\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.mlab as mlab\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import *\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.models import *\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"..\") # Adds higher directory to python modules path.\n",
    "from data.preprocess import *\n",
    "data = preprocess(path='../data/raw/CD_PD.mat')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Splitting the data into test/train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_by_patient(data):\n",
    "    \n",
    "    bound = int(0.8*len(data))\n",
    "    idx_train = np.random.choice(len(data), bound , replace=False)\n",
    "    idx_test = np.array([e for e in np.arange(len(data)) if e not in idx_train])\n",
    "#     print(idx_train, idx_test)\n",
    "    return data[idx_train], data[idx_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(y_true, y_pred, verbose=False):\n",
    "    \n",
    "    '''\n",
    "    This function return default 4 metrics\n",
    "    '''\n",
    "    #cast to np.int32\n",
    "    y_true = y_true.astype(dtype=np.int32)\n",
    "    y_pred = y_pred.astype(dtype=np.int32)\n",
    "    \n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    rec = recall_score(y_true, y_pred)\n",
    "    prec= precision_score(y_true, y_pred)\n",
    "    f1  = f1_score(y_true, y_pred)\n",
    "    \n",
    "    if verbose:\n",
    "        print('--------evaluation---------')\n",
    "        print('accuracy:', acc)\n",
    "        print('recall:', rec)\n",
    "        print('precision:', prec)\n",
    "        print('f1 score:', f1)\n",
    "        print('---------------------------')\n",
    "        \n",
    "    return acc, rec, prec, f1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This baseline will uniformly random the label (0 = No, 1 = Yes), and used to compare with other models. Other models should exceed this baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------evaluation---------\n",
      "accuracy: 0.5\n",
      "recall: 0.4583333333333333\n",
      "precision: 0.6111111111111112\n",
      "f1 score: 0.5238095238095238\n",
      "---------------------------\n"
     ]
    }
   ],
   "source": [
    "data_train, data_test = split_by_patient(data)\n",
    "y_true = data_test['diagnosis']\n",
    "y_pred = np.array([np.random.choice(2) for i in range(len(y_true))])\n",
    "\n",
    "acc, rec, prec, f1 = evaluate(y_true, y_pred, verbose=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
