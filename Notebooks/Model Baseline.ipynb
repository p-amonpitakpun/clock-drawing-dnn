{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.io import loadmat\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.mlab as mlab\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import *\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.models import *\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"..\") # Adds higher directory to python modules path.\n",
    "from data.preprocess import *\n",
    "data = preprocess(path='../data/raw/CD_PD.mat')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Splitting the data into test/train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.preprocess import split_by_patient\n",
    "# def split_by_patient(data):\n",
    "    \n",
    "#     bound = int(0.8*len(data))\n",
    "#     idx_train = np.random.choice(len(data), bound , replace=False)\n",
    "#     idx_test = np.array([e for e in np.arange(len(data)) if e not in idx_train])\n",
    "# #     print(idx_train, idx_test)\n",
    "#     return data[idx_train], data[idx_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluation.benchmark import *\n",
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "# from sklearn.metrics import *\n",
    "# from .evaluate import evaluate\n",
    "\n",
    "# def run_benchmark(get_split_data,\n",
    "#               get_model,\n",
    "#               data,\n",
    "#               k=1, \n",
    "#               model_name='Model',\n",
    "#               verbose=False,\n",
    "#               figsize=(15,15)\n",
    "#              ):\n",
    "    \n",
    "#     '''\n",
    "#     This function will run performance benchmark on the given model.\n",
    "#     Input:\n",
    "#         get_split_data : function to return sampled test train data\n",
    "#         get_model : function to return a model to run benchmark on\n",
    "#         data : data numpy\n",
    "#         k : iteration to run the tests (higher k ~ better avg. value)\n",
    "#         model_name (default='Model') : Name for the model, used in plotting ROC Curves\n",
    "#         verbose (default=False) : If true, then display benchmarking result/logs in each iteration\n",
    "        \n",
    "#     Output:\n",
    "#         models : list of models which were generated & trained during the benchmarking\n",
    "#     '''\n",
    "    \n",
    "#     acc, rec, prec, f1 = [], [], [], []\n",
    "#     fp, tp, thresh = [], [], []\n",
    "#     mean_tp, _tp = [], []\n",
    "#     domain = np.linspace(0, 1, 100)\n",
    "#     aucs = []\n",
    "#     models = []\n",
    "    \n",
    "#     for i in range(k):\n",
    "#         if verbose:\n",
    "#             print('ITER:',i)\n",
    "#         x_test, x_train, y_test, y_train = get_split_data(data) #data should randomly shuffle\n",
    "    \n",
    "#         #train & predict\n",
    "#         model = get_model()\n",
    "#         model.fit(x_train, y_train, epochs=100,verbose=0)\n",
    "#         models.append(model) #store model\n",
    "#         y_pred = model.predict(x_test)\n",
    "\n",
    "#         #metric eval for thresh = 0.5\n",
    "#         y2 = np.array(y_pred)\n",
    "#         y2[y2 >= 0.5] = 1\n",
    "#         y2[y2 < 0.5]  = 0\n",
    "#         t = evaluate(y_test, y2,verbose=verbose)\n",
    "\n",
    "#         #append the evaluation results to log\n",
    "#         acc.append(t[0])\n",
    "#         rec.append(t[1])\n",
    "#         prec.append(t[2])\n",
    "#         f1.append(t[3])\n",
    "\n",
    "#         #ROC for other thresh\n",
    "#         f, t, p = roc_curve(y_test, y_pred)\n",
    "#         _tp.append(np.interp(domain, f, t))\n",
    "\n",
    "#         #AUC\n",
    "#         a = auc(f, t)\n",
    "\n",
    "#         #logging\n",
    "#         fp.append(f)\n",
    "#         tp.append(t)\n",
    "#         thresh.append(p)\n",
    "#         aucs.append(a)\n",
    "        \n",
    "#     if verbose:\n",
    "#         print('-----thresh=0.5--------')\n",
    "#         print('mean acc:',np.mean(acc))\n",
    "#         print('mean recall:',np.mean(rec))\n",
    "#         print('mean precision:',np.mean(prec))\n",
    "#         print('mean f1:',np.mean(f1))\n",
    "#         print('-----------------------')\n",
    "#         plt.title('Evaluation for 10 runs (thresh=0.5)')\n",
    "#         plt.plot(np.arange(k), acc, label='acc')\n",
    "#         plt.plot(np.arange(k), rec, label='recall')\n",
    "#         plt.plot(np.arange(k), prec,label='precision')\n",
    "#         plt.plot(np.arange(k), f1,  label='f1')\n",
    "#         plt.xlabel('iteration')\n",
    "#         plt.ylabel('score')\n",
    "#         plt.legend()\n",
    "#         plt.show()\n",
    "        \n",
    "#     #Calculate mean and std of roc curve\n",
    "#     mean_tp = np.mean(_tp, axis=0)\n",
    "#     std_tp = np.std(_tp, axis=0)\n",
    "#     upper_tp = mean_tp + std_tp\n",
    "#     lower_tp = mean_tp - std_tp\n",
    "#     mean_auc = auc(domain, mean_tp)\n",
    "    \n",
    "#     #ref: https://scikit-learn.org/stable/auto_examples/model_selection/plot_roc_crossval.html#sphx-glr-auto-examples-model-selection-plot-roc-crossval-py\n",
    "#     plt.figure(figsize=figsize)\n",
    "#     plt.title('ROC Curve')\n",
    "#     plt.xlabel('fpr')\n",
    "#     plt.ylabel('tpr')\n",
    "#     plt.plot([0,1],[0,1],label='Chance', linestyle='dashed',color='red')\n",
    "    \n",
    "#     #plot ROC Curve\n",
    "#     for i in range(k):\n",
    "#         plt.plot(fp[i], tp[i], label='%s iteration:%d AUC=%.2f' % (model_name,i,aucs[i]), alpha=0.2)\n",
    "#     plt.plot(domain, mean_tp, color='blue', alpha=1, label='average AUC=%.2f' % mean_auc)\n",
    "#     # plt.plot(domain, upper_tp, color='gray', alpha=0.1)\n",
    "#     # plt.plot(domain, lower_tp, color='gray', alpha=0.1)\n",
    "#     plt.fill_between(domain, upper_tp, lower_tp, color='grey', alpha=.2, label=r'$\\pm$ 1 std. dev.')\n",
    "#     plt.legend()\n",
    "#     plt.show()\n",
    "    \n",
    "#     return models\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This baseline will uniformly random the label (0 = No, 1 = Yes), and used to compare with other models. Other models should exceed this baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------evaluation---------\n",
      "accuracy: 0.625\n",
      "recall: 0.625\n",
      "precision: 0.7142857142857143\n",
      "f1 score: 0.6666666666666666\n",
      "---------------------------\n"
     ]
    }
   ],
   "source": [
    "data_train, data_test = split_by_patient(data)\n",
    "y_true = data_test['diagnosis'].astype(np.float32)\n",
    "y_pred = np.array([np.random.choice(2) for i in range(len(y_true))])\n",
    "acc, rec, prec, f1 = evaluate(y_true, y_pred, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# All One Baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This baseline will predict y = 1 for any x."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------evaluation---------\n",
      "accuracy: 0.675\n",
      "recall: 1.0\n",
      "precision: 0.675\n",
      "f1 score: 0.8059701492537313\n",
      "---------------------------\n"
     ]
    }
   ],
   "source": [
    "data_train, data_test = split_by_patient(data)\n",
    "y_true = data_test['diagnosis'].astype(np.float32)\n",
    "y_pred = np.ones(shape=(y_true.shape))\n",
    "acc, rec, prec, f1 = evaluate(y_true, y_pred, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# All Zero Baseline\n",
    "\n",
    "This baseline will predict y = 0 for any x."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------evaluation---------\n",
      "accuracy: 0.35\n",
      "recall: 0.0\n",
      "precision: 0.0\n",
      "f1 score: 0.0\n",
      "---------------------------\n"
     ]
    }
   ],
   "source": [
    "data_train, data_test = split_by_patient(data)\n",
    "y_true = data_test['diagnosis'].astype(np.float32)\n",
    "y_pred = np.zeros(shape=(y_true.shape))\n",
    "acc, rec, prec, f1 = evaluate(y_true, y_pred, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will run the model training & inference k times and evaluate the average performance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluate.benchmark import run_benchmark\n",
    "# def run_benchmark(get_split_data,\n",
    "#               get_model,\n",
    "#               data,\n",
    "#               k=1, \n",
    "#               model_name='Model',\n",
    "#               verbose=False\n",
    "#              ):\n",
    "    \n",
    "#     '''\n",
    "#     This function will run performance benchmark on the given model.\n",
    "#     Input:\n",
    "#         get_split_data : function to return sampled test train data\n",
    "#         get_model : function to return a model to run benchmark on\n",
    "#         k : iteration to run the tests (higher k ~ better avg. value)\n",
    "#         model_name (default='Model') : Name for the model, used in plotting ROC Curves\n",
    "#         verbose (default=False) : If true, then display benchmarking result/logs in each iteration\n",
    "        \n",
    "#     Output:\n",
    "#         models : list of models which were generated & trained during the benchmarking\n",
    "#     '''\n",
    "    \n",
    "#     acc, rec, prec, f1 = [], [], [], []\n",
    "#     fp, tp, thresh = [], [], []\n",
    "#     mean_tp, _tp = [], []\n",
    "#     domain = np.linspace(0, 1, 100)\n",
    "#     aucs = []\n",
    "#     models = []\n",
    "    \n",
    "#     for i in range(k):\n",
    "#         if verbose:\n",
    "#             print('ITER:',i)\n",
    "#         x_test, x_train, y_test, y_train = get_split_data(data) #data should randomly shuffle\n",
    "    \n",
    "#         #train & predict\n",
    "#         model = get_model()\n",
    "#         model.fit(x_train, y_train, epochs=100,verbose=0)\n",
    "#         models.append(model) #store model\n",
    "#         y_pred = model.predict(x_test)\n",
    "\n",
    "#         #metric eval for thresh = 0.5\n",
    "#         y2 = np.array(y_pred)\n",
    "#         y2[y2 >= 0.5] = 1\n",
    "#         y2[y2 < 0.5]  = 0\n",
    "#         t = evaluate(y_test, y2,verbose=verbose)\n",
    "\n",
    "#         #append the evaluation results to log\n",
    "#         acc.append(t[0])\n",
    "#         rec.append(t[1])\n",
    "#         prec.append(t[2])\n",
    "#         f1.append(t[3])\n",
    "\n",
    "#         #ROC for other thresh\n",
    "#         f, t, p = roc_curve(y_test, y_pred)\n",
    "#         _tp.append(np.interp(domain, f, t))\n",
    "\n",
    "#         #AUC\n",
    "#         a = auc(f, t)\n",
    "\n",
    "#         #logging\n",
    "#         fp.append(f)\n",
    "#         tp.append(t)\n",
    "#         thresh.append(p)\n",
    "#         aucs.append(a)\n",
    "        \n",
    "#     if verbose:\n",
    "#         print('-----thresh=0.5--------')\n",
    "#         print('mean acc:',np.mean(acc))\n",
    "#         print('mean recall:',np.mean(rec))\n",
    "#         print('mean precision:',np.mean(prec))\n",
    "#         print('mean f1:',np.mean(f1))\n",
    "#         print('-----------------------')\n",
    "#         plt.title('Evaluation for 10 runs (thresh=0.5)')\n",
    "#         plt.plot(np.arange(k), acc, label='acc')\n",
    "#         plt.plot(np.arange(k), rec, label='recall')\n",
    "#         plt.plot(np.arange(k), prec,label='precision')\n",
    "#         plt.plot(np.arange(k), f1,  label='f1')\n",
    "#         plt.xlabel('iteration')\n",
    "#         plt.ylabel('score')\n",
    "#         plt.legend()\n",
    "#         plt.show()\n",
    "        \n",
    "#     #Calculate mean and std of roc curve\n",
    "#     mean_tp = np.mean(_tp, axis=0)\n",
    "#     std_tp = np.std(_tp, axis=0)\n",
    "#     upper_tp = mean_tp + std_tp\n",
    "#     lower_tp = mean_tp - std_tp\n",
    "#     mean_auc = auc(domain, mean_tp)\n",
    "    \n",
    "#     #ref: https://scikit-learn.org/stable/auto_examples/model_selection/plot_roc_crossval.html#sphx-glr-auto-examples-model-selection-plot-roc-crossval-py\n",
    "#     plt.figure(figsize=(15,15))\n",
    "#     plt.title('ROC Curve')\n",
    "#     plt.xlabel('fpr')\n",
    "#     plt.ylabel('tpr')\n",
    "#     plt.plot([0,1],[0,1],label='Chance', linestyle='dashed',color='red')\n",
    "    \n",
    "#     #plot ROC Curve\n",
    "#     for i in range(k):\n",
    "#         plt.plot(fp[i], tp[i], label='%s iteration:%d AUC=%.2f' % (model_name,i,aucs[i]), alpha=0.2)\n",
    "#     plt.plot(domain, mean_tp, color='blue', alpha=1, label='average AUC=%.2f' % mean_auc)\n",
    "#     # plt.plot(domain, upper_tp, color='gray', alpha=0.1)\n",
    "#     # plt.plot(domain, lower_tp, color='gray', alpha=0.1)\n",
    "#     plt.fill_between(domain, upper_tp, lower_tp, color='grey', alpha=.2, label=r'$\\pm$ 1 std. dev.')\n",
    "#     plt.legend()\n",
    "#     plt.show()\n",
    "    \n",
    "#     return models\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
